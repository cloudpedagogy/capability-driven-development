# AI-Assisted Reflection

This directory contains **optional, human-led reflective resources** that support thinking during **Capability-Driven Development (CDD)**.

These materials explore how AI can be used as a **reflective aid** — not a decision-maker, evaluator, or authority — during the design, review, and evolution of AI-enabled systems.

They are intentionally **secondary** to:
- the CDD method (`/method`)
- the design artifacts (`/design-artifacts`)
- human judgement and institutional responsibility

---

## Why This Folder Exists

Designing AI-enabled systems often involves:
- ambiguity
- incomplete information
- ethical tension
- distributed responsibility
- evolving assumptions

In these conditions, AI can sometimes help humans:
- surface blind spots
- reframe questions
- articulate uncertainty
- test clarity of intent

This folder exists to explore **when and how that reflective use of AI is appropriate** — and, just as importantly, when it is not.

---

## What These Resources Are

The files in this directory are:

- **guided reflective essays**, not prompt packs
- explicitly **human-led**
- designed to slow thinking, not accelerate decisions
- aligned with capability, governance, and responsibility
- safe to use in education, research, and public-interest contexts

They note *where* AI can assist reflection, and *where it actively distorts it*.

---

## What These Resources Are Not

These resources are **not**:

- decision-making tools
- evaluation or approval mechanisms
- governance frameworks
- prompt-engineering recipes
- substitutes for professional or institutional judgement

If AI outputs begin to:
- justify decisions
- define governance
- assess acceptability
- optimise outcomes

then these resources are being misused.

---

## How to Use This Folder

These reflections are best used:

- **after** initial human thinking, not before it  
- **alongside** the relevant design artifact  
- **individually or in small groups**, not automatically  
- **periodically**, not continuously  

They should be treated as **optional pauses for reflection**, not as required steps.

Skipping them is acceptable.  
Using them uncritically is not.

---

## Relationship to Capability-Driven Development

Each reflection aligns with a stage of the CDD lifecycle, but does not define it.

| Reflection Focus | Supports |
|------------------|---------|
| Capability intent | `design-artifacts/system-capability-brief.md` |
| Ethics, equity & risk | `design-artifacts/ethics-equity-and-risk-notes.md` |
| Governance design | `design-artifacts/governance-and-oversight-plan.md` |
| Evaluation & iteration | `design-artifacts/evaluation-and-learning-log.md` |

If AI-assisted reflection conflicts with documented human judgement,  
**human judgement prevails**.

---

## Contents

- `capability-intent.md`  
  Reflective support for clarifying and stress-testing system purpose and capability intent.

- `ethics-and-risk.md`  
  Reflection on ethical tension, equity implications, and second-order risk.

- `governance-design.md`  
  Reflection on responsibility, oversight, escalation, and accountability in practice.

- `evaluation-and-iteration.md`  
  Reflection on learning, iteration, restraint, and whether systems should change — or end.

---

## A Note on Responsibility

AI-assisted reflection does **not** transfer responsibility to AI.

Responsibility remains with:
- designers
- developers
- educators
- researchers
- governance bodies
- institutions

These resources exist to **support responsible thinking**, not to automate it.

---

## Summary

This folder exists to support one principle:

> **AI may help humans reflect more clearly — but it must never decide, govern, or justify on their behalf.**

If reflection becomes reassurance, optimisation, or delegation,  
pause the design and return to human judgement.
