# AI-Assisted Reflection: Evaluation and Iteration

_Status: Optional · Secondary · Human-Led_

---

## Purpose and Limits

This document supports **human-led reflection on evaluation, learning, and iteration** for AI-enabled systems.

It exists to help teams:
- learn from real-world use over time
- detect capability erosion as well as improvement
- distinguish learning from optimisation
- decide whether iteration is justified, limited, or inappropriate

AI may be used here as a **sense-making aid**, not as an evaluator, optimiser, or decision-maker.

This document does **not** replace:
- the Evaluation and Learning Log
- governance review processes
- human judgement about continuation or change

If evaluation becomes optimisation, stop using AI.

---

## When This Reflection Is Useful

This reflection is most useful:

- after initial deployment or pilot use  
- when patterns of use stabilise  
- when outcomes feel “good” but unease remains  
- when iteration is proposed without clear justification  
- when systems begin to feel embedded or taken for granted  

It is **not** intended to justify expansion or permanence.

---

## Core Human-Led Reflection on Learning

Before involving AI, reflect carefully on the questions below.

### 1. What are we actually learning from use?

Distinguish between:
- what is happening technically
- what is happening to human judgement
- what is happening to responsibility
- what is happening to institutional practice

Ask:
- What surprises have emerged?
- What assumptions no longer hold?
- What is being learned unintentionally?

Learning is not always positive.

---

### 2. Is capability being strengthened or eroded?

Reflect on:
- whether humans feel more confident or more dependent
- whether judgement is exercised or deferred
- whether escalation and contestation still occur
- whether accountability feels clearer or blurrier

Capability erosion often appears as *smoothness*, not failure.

---

### 3. What signals matter more than metrics?

Consider qualitative signals such as:
- changes in behaviour
- reduced challenge or questioning
- increased reliance on defaults
- avoidance of responsibility
- narrowing of acceptable outcomes

Ask:
- What would concern us even if performance metrics looked good?
- What would metrics fail to capture?

---

### 4. When is iteration appropriate — and when is it not?

Iteration is not always the right response.

Reflect on:
- whether problems indicate design flaws or fundamental misfit
- whether iteration would increase complexity or risk
- whether human processes should be strengthened instead
- whether withdrawal or scope reduction is more responsible

Iteration should be a choice, not a reflex.

---

## Optional AI-Assisted Reflection

Once human learning is articulated, AI may be used cautiously to:

- summarise recurring themes in observations
- surface tensions or contradictions
- suggest alternative interpretations of patterns
- prompt reflection on long-term effects

Example prompts (adapt freely):

- “What patterns or tensions appear in these observations?”
- “What assumptions seem to be challenged by current use?”
- “What long-term effects might follow if current patterns continue?”

---

## How to Interpret AI Responses

Treat AI outputs as:
- hypotheses
- reframings
- prompts for deeper discussion

Do **not** treat them as:
- evaluations of success or failure
- justifications for iteration
- optimisation recommendations
- indicators that change is required

If AI responses feel:
- efficiency-focused
- overly confident
- detached from lived practice
- dismissive of uncertainty

they should be ignored.

---

## Failure Modes and Warnings

Stop using AI-assisted evaluation reflection if:

- learning is reduced to performance improvement
- iteration is justified by convenience or momentum
- negative signals are reframed as “edge cases”
- expansion is assumed rather than questioned
- AI outputs begin to drive design decisions

Learning requires restraint as much as action.

---

## Relationship to Capability-Driven Development

This reflection supports — but does not replace —:

- `design-artifacts/evaluation-and-learning-log.md`
- `design-artifacts/governance-and-oversight-plan.md`
- `design-artifacts/retirement-and-transition-notes.md`

If learning suggests that a system should not continue,  
that conclusion must be taken seriously.

Iteration is one possible outcome — not the default.

---

## Summary

This reflection exists to support one final question:

> **What are we learning about this system’s effect on human and institutional capability — and what responsibility does that learning create?**

AI can help surface patterns.  
It cannot decide what should change, or whether change should occur.

If learning feels effortless, slow down.
