# AI-Assisted Reflection: Ethics, Equity, and Risk

_Status: Optional · Secondary · Human-Led_

---

## Purpose and Limits

This document supports **human-led ethical, equity, and risk reflection** during the design and evolution of AI-enabled systems.

It exists to:
- surface ethical tensions early
- consider uneven or indirect impacts
- resist deferring ethics until after deployment
- support responsible judgement under uncertainty

AI may be used here as a **reflective aid**, not as a moral authority or risk assessor.

This document does **not** replace:
- ethical judgement
- institutional governance
- professional responsibility
- the Ethics, Equity, and Risk Notes design artifact

If ethical reflection becomes procedural, pause and return to human discussion.

---

## When This Reflection Is Useful

This reflection is particularly useful:

- when systems affect people indirectly or asymmetrically  
- when decisions involve vulnerability, power imbalance, or care  
- when automation may influence judgement, not just efficiency  
- when risks are diffuse, slow, or hard to quantify  
- when “no obvious harm” is being used as justification  

It is **not** intended to certify ethical acceptability.

---

## Core Human-Led Ethical Reflection

Before involving AI, reflect deliberately on the questions below.

### 1. Who could be affected — beyond direct users?

Ethical impact rarely stops at intended users.

Consider:
- people represented in data
- individuals subject to decisions or categorisation
- groups indirectly influenced by system outputs
- those with limited ability to contest outcomes

Ask:
- Who is visible to the system?
- Who is invisible but still affected?

---

### 2. Where does power operate in this system?

AI systems often redistribute power subtly.

Reflect on:
- who gains influence or efficiency
- who loses discretion or voice
- where authority becomes opaque
- whose judgement is amplified or sidelined

Power shifts matter even when outcomes appear neutral.

---

### 3. What kinds of harm are plausible here?

Consider harms beyond obvious failure:

- misclassification or misinterpretation
- normalisation of surveillance or judgement
- erosion of trust or autonomy
- cumulative disadvantage over time
- chilling effects on behaviour

Some harms emerge slowly and unevenly.

---

### 4. Where might equity be compromised?

Equity risks often arise through:
- unequal access to contestation
- reliance on dominant norms or language
- assumptions about capacity, availability, or context
- differential error rates or confidence signals

Ask:
- Who benefits most?
- Who bears the cost when the system is wrong?
- Who has the least ability to challenge outcomes?

---

### 5. What ethical trade-offs are being accepted?

No system is ethically neutral.

Reflect on:
- which risks are being tolerated
- who is accepting them
- whether trade-offs are explicit or implicit
- whether alternatives were seriously considered

Unacknowledged trade-offs undermine defensibility.

---

## Optional AI-Assisted Reflection

Once human concerns are articulated, AI may be used cautiously to:

- suggest categories of ethical risk
- surface second-order or indirect effects
- generate alternative framings of harm
- test whether important stakeholders have been overlooked

Example prompts (adapt freely):

- “What indirect ethical risks might arise from this system’s use?”
- “Who might be affected even if they never interact with the system?”
- “What equity concerns could emerge over time rather than immediately?”

---

## How to Interpret AI Responses

Treat AI outputs as:
- prompts for further thought
- possible blind spots
- alternative lenses

Do **not** treat them as:
- ethical judgements
- risk ratings
- acceptability thresholds
- substitutes for lived context

If AI responses feel:
- generic
- abstract
- overly confident
- detached from real consequences

they should be set aside.

---

## Failure Modes and Warnings

Stop using AI-assisted ethical reflection if:

- AI starts framing risk as optimisation
- harms are reduced to metrics or probabilities
- ethical issues are treated as edge cases
- responsibility feels diluted rather than clarified
- reflection becomes performative

Ethics requires discomfort.  
AI tends to smooth it away.

---

## Relationship to Capability-Driven Development

This reflection supports — but does not replace —:

- `design-artifacts/ethics-equity-and-risk-notes.md`
- `design-artifacts/human-ai-boundary-map.md`
- `design-artifacts/governance-and-oversight-plan.md`

If AI-assisted reflection conflicts with documented ethical judgement,  
**human judgement prevails**.

If ethical concerns cannot be resolved, reconsider the system’s scope or existence.

---

## Summary

This reflection exists to support one critical question:

> **Who could be harmed here, how, and why might that be easy to miss?**

AI can help surface possibilities.  
It cannot tell you what is acceptable.

If ethical clarity depends on AI reassurance, pause the design.
