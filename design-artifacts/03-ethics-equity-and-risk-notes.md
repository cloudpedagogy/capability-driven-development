# Ethics, Equity, and Risk Notes

_Status: Draft_

---

## Purpose of These Notes

This document supports **early, reflective consideration of ethical, equity, and risk implications** arising from an AI-enabled system.

It exists to:
- surface foreseeable harms and tensions
- consider who may be advantaged or disadvantaged
- prevent ethical issues being deferred until after deployment
- inform boundary, governance, and design decisions

These notes are **not exhaustive** and are not intended to predict every risk.  
They are a **thinking aid**, not a compliance artefact.

---

## System Context

**System name:**  
*(As per the System Capability Brief)*

**Context of use:**  
*(Education, research, public service, professional practice, etc.)*

**Primary capability intent:**  
*(Restated briefly from the Capability Brief)*

---

## Ethical Sensitivities

What ethical considerations are inherent in this system’s purpose or use?

Consider questions such as:
- Does the system influence decisions that affect people’s opportunities, rights, or wellbeing?
- Does it operate in contexts of power imbalance or vulnerability?
- Does it risk normalising surveillance, judgement, or exclusion?
- Does it shape behaviour through nudging or prioritisation?

Describe the ethical sensitivities in plain language.

---

## Equity Considerations

Who might be **unequally affected** by this system?

Consider:
- access and exclusion
- differential impact across groups
- cultural, linguistic, or contextual bias
- indirect or downstream effects

Questions to reflect on:
- Who benefits most from this system?
- Who may be disadvantaged, overlooked, or misinterpreted?
- Are some users more able to contest or override system outputs than others?

Equity risks may be subtle and cumulative.

---

## Risk of Capability Erosion

How could this system undermine the capabilities it is meant to support?

Consider risks such as:
- over-reliance on automation
- reduced exercise of professional judgement
- diminished accountability
- loss of discretion or care
- narrowing of acceptable decisions

These risks often emerge over time rather than immediately.

---

## Misuse and Scope Creep

How might this system be misused or extended beyond its original intent?

Examples include:
- use for performance monitoring rather than support
- repurposing data for unrelated oversight
- expansion of automation without boundary review
- application in higher-risk contexts without redesign

Identify plausible misuse scenarios, even if unintended.

---

## Uncertainty and Error

Where is uncertainty likely to arise?

Consider:
- ambiguous inputs
- incomplete data
- contextual variation
- model limitations
- interpretation errors

How might uncertainty affect:
- fairness
- trust
- decision quality
- responsibility?

Uncertainty should be visible and actionable, not hidden.

---

## Mitigation and Design Responses

How will identified risks be **addressed through design**, not just policy?

Consider:
- human–AI boundary design
- escalation and fallback mechanisms
- contestability and override
- governance and review processes
- limits on scope or use

Mitigations should preserve capability rather than restrict use unnecessarily.

---

## Residual Risk and Acceptability

Which risks remain even after mitigation?

For each:
- Why is the risk considered acceptable?
- Who accepts responsibility for it?
- How will it be monitored over time?

Explicitly naming residual risk supports defensible decision-making.

---

## Relationship to Other Design Artifacts

These notes:
- build on the **System Capability Brief**
- inform the **Human–AI Boundary Map**
- shape the **Governance and Oversight Plan**
- guide evaluation and learning

Ethical considerations should be revisited whenever system scope or behaviour changes.

---

## Review and Reflection

**Date created:**  
**Created by:**  

**Last reviewed:**  
**Review notes:**  

Ethical and equity risks evolve with context.  
This document should be revisited regularly.

---

## Summary

These Ethics, Equity, and Risk Notes exist to answer one question:

> **What could go wrong here, for whom, and why does that matter?**

Responsible system design depends on confronting these questions early — and revisiting them often.
