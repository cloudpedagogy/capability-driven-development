# Evaluation and Learning Log

_Status: Draft_

---

## Purpose of This Log

This document supports **ongoing evaluation, reflection, and learning** from the use of an AI-enabled system.

It exists to ensure that:
- system behaviour is reviewed over time
- capability impacts are observed, not assumed
- emerging risks or unintended effects are detected
- learning informs iteration, governance, or retirement decisions

This log is **continuous and reflective**, not a one-off evaluation.

---

## System Context

**System name:**  
*(As per the System Capability Brief)*

**Context of use:**  
*(Education, research, public service, professional practice, etc.)*

**Primary capability intent:**  
*(Restated briefly from the Capability Brief)*

---

## What Is Being Evaluated

This log focuses on **capability outcomes**, not just technical performance.

Areas of attention may include:
- exercise of human judgement
- use of escalation and override
- distribution of workload or attention
- equity of outcomes or experience
- alignment with original capability intent

Not all areas need to be evaluated at once.

---

## Signals and Observations

Record qualitative and quantitative signals that may indicate how the system is functioning in practice.

Examples:
- recurring patterns in use or misuse
- feedback from users or affected parties
- frequency and nature of overrides or escalations
- areas of confusion, friction, or reliance
- unexpected benefits or harms

Observations should be descriptive, not defensive.

---

## Learning and Interpretation

What do these observations suggest?

Consider:
- Is the system supporting or eroding capability?
- Are boundaries being respected in practice?
- Are new risks or inequities emerging?
- Are assumptions from the design phase still valid?

Learning may be tentative or incomplete.

---

## Actions and Responses

What actions, if any, are being taken in response to learning?

Examples:
- boundary clarification or adjustment
- changes to governance or oversight
- user guidance or training
- technical or workflow changes
- decision to pause, limit, or expand use

Not all learning requires immediate action.

---

## Escalations and Incidents

Record any significant incidents, challenges, or escalations.

For each:
- brief description
- how it was handled
- what was learned
- whether further review is needed

This supports accountability and institutional memory.

---

## Review Cycle

**Review period:**  
*(e.g. quarterly, termly, annually)*

**Review participants:**  
*(Roles involved in reflection and review)*

**Next planned review:**  

Evaluation should be proportionate to system impact and risk.

---

## Relationship to Other Design Artifacts

This log:
- tests assumptions made in the **System Capability Brief**
- reveals pressure points in the **Humanâ€“AI Boundary Map**
- informs updates to the **Ethics, Equity, and Risk Notes**
- triggers review of the **Governance and Oversight Plan**
- may inform **Retirement and Transition Notes**

Learning should flow across artefacts.

---

## Summary

This Evaluation and Learning Log exists to answer one question over time:

> **What are we learning about how this system affects human and institutional capability?**

Responsible systems improve not because they are perfect at launch,  
but because they are **designed to learn**.
