# Human–AI Boundary Map

_Status: Draft_

---

## Purpose of This Map

This document makes **human–AI boundaries explicit** for an AI-enabled system.

It exists to ensure that:
- authority and responsibility are clearly located
- AI assistance does not silently become AI decision-making
- escalation and override are designed, not improvised
- accountability remains human and defensible

This is a **design artefact**, not a compliance document.  
Its purpose is clarity, not exhaustiveness.

---

## System Context

**System name:**  
*(Should match the System Capability Brief)*

**Context of use:**  
*(Education, research, public service, professional practice, etc.)*

**Primary purpose of the system:**  
*(In one sentence, restated from the Capability Brief)*

---

## Core Boundary Principle

Before mapping tasks or functions, state the **boundary rule** for this system:

> **What must humans always retain authority over, regardless of automation?**

Examples:
- final decisions
- ethical judgement
- escalation and resolution
- interpretation of ambiguous cases

This principle constrains everything that follows.

---

## Task and Decision Mapping

List the key activities or decision points in the system.

For each, clarify **who does what**, **who decides**, and **what happens under uncertainty**.

### Activity / Decision Area 1

**Description:**  
*(What is happening at this point in the system?)*

- AI role (if any):  
  *(e.g. summarise, classify, surface signals)*

- Human role:  
  *(e.g. interpret, decide, review, escalate)*

- Who has decision authority?  
  *(Human role or body)*

- Is this reversible?  
  *(Yes / No — explain briefly)*

- What happens if confidence is low or context is unclear?  
  *(Fallback or escalation path)*

---

### Activity / Decision Area 2

*(Repeat structure as needed — do not aim for completeness, aim for clarity)*

---

## Escalation Pathways

Describe how **uncertainty, disagreement, or risk** is handled.

Consider:
- What triggers escalation?
- Who receives escalated cases?
- Is escalation easy and non-punitive?
- Can humans escalate even if the system does not?

Escalation should be treated as **responsible action**, not failure.

---

## Override and Contestation

Describe how humans can **challenge or override** system behaviour.

Questions to consider:
- Can AI outputs be ignored or rejected without friction?
- Are alternative actions available?
- Is override visible and legitimate?
- Are overrides logged for learning (not blame)?

If override is difficult, explain why and what safeguards exist.

---

## Boundary Risks and Tensions

Identify where boundaries may be at risk of erosion.

Examples:
- approval steps becoming rubber-stamping
- AI outputs becoming default choices
- time pressure discouraging human review
- escalation routes being bypassed

These risks should inform later governance and evaluation artefacts.

---

## Relationship to Other Design Artifacts

This boundary map:
- operationalises the **System Capability Brief**
- informs the **Ethics, Equity, and Risk Notes**
- constrains the **Governance and Oversight Plan**
- shapes **System Design Decisions**

If boundaries change later, those changes should be documented and justified.

---

## Review and Revision

**Date created:**  
**Created by:**  

**Last reviewed:**  
**Review notes:**  

Boundary design should be revisited:
- when scope changes
- when automation increases
- when incidents or challenges occur

---

## Summary

This Human–AI Boundary Map exists to answer one question clearly:

> **Who has authority here, and how is that authority protected in practice?**

If the answer becomes unclear, the system is no longer capability-driven.
