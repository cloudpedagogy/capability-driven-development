# Ethics, Equity, and Risk

_Status: Draft_

---

## Purpose of This Step

This step identifies and addresses **ethical, equity, and risk considerations** that arise from the intended capabilities of a system and the human‚ÄìAI boundaries defined in earlier steps.

The purpose is to ensure that:
- foreseeable harms are identified *before* implementation
- equity impacts are considered as design constraints, not side effects
- risk is treated as a systemic property, not just a technical one
- ethical reasoning informs system behaviour, not just documentation

In Capability-Driven Development, ethics and risk are **design inputs**, not post-deployment reviews.

---

## Why Ethics, Equity, and Risk Must Be Addressed Early

In many AI-enabled systems, ethical and risk considerations are handled:
- after system architecture is fixed
- through high-level principles without operational impact
- via policy documents disconnected from system behaviour

This leads to systems that are:
- efficient but harmful
- scalable but exclusionary
- technically correct but ethically indefensible

CDD treats these outcomes as **design failures**, not inevitable trade-offs.

---

## What This Step Covers

This step examines how the system may:

- disadvantage or exclude certain groups
- amplify existing inequalities or biases
- shift risk or burden onto individuals
- create new forms of dependency or surveillance
- undermine trust, autonomy, or professional judgement

It focuses on **consequences**, not intentions.

---

## Relationship to Earlier Steps

Ethics, equity, and risk considerations emerge directly from:

- **Capability intent**  
  What capabilities are prioritised, protected, or constrained.

- **Human‚ÄìAI boundaries**  
  Where automation, delegation, and decision authority are located.

If ethical or equity risks cannot be mitigated within the current boundaries, those boundaries must be revisited.

---

## Core Design Questions

Designers should be able to address the following questions explicitly.

### Ethical impact
- What harms could reasonably arise from this system‚Äôs use or misuse?
- Who bears the consequences when the system fails or is wrong?
- Are there decisions or contexts where automation should be limited or prohibited?

### Equity and inclusion
- Who benefits most from this system?
- Who may be disadvantaged, excluded, or disproportionately affected?
- Does the system assume access, literacy, or capacity that cannot be guaranteed?

### Risk distribution
- Where does risk sit ‚Äî with individuals, professionals, or the institution?
- Does the system shift responsibility without authority?
- Are some users exposed to risk without meaningful recourse?

### Foreseeable misuse
- How could this system be misused, repurposed, or overextended?
- What pressures might encourage inappropriate reliance on automation?
- What safeguards limit scope creep?

---

## Common Failure Modes at This Stage

Typical failures include:

- treating ethics as abstract principles rather than design constraints
- focusing only on bias in data, not in system behaviour
- assuming users will compensate for system weaknesses
- overlooking cumulative or long-term impacts
- deferring difficult trade-offs to ‚Äúfuture governance‚Äù

These failures often become visible only when systems are contested or scrutinised.

---

## Outputs of This Step

The outcome of this step should be a **documented ethics, equity, and risk analysis**, typically captured using the **Risk and Misuse Register** in `/design-artifacts`.

At minimum, this output should include:

- identified ethical and equity risks
- affected groups or stakeholders
- potential severity and likelihood
- mitigation strategies or design constraints
- risks that are explicitly accepted (with rationale)

Unmitigated or unacceptable risks must be escalated and may require revisiting earlier steps.

---

## Relationship to Governance and Oversight

Ethical and equity considerations directly inform:

- what must be logged or auditable
- which decisions require review
- where human oversight is mandatory
- how accountability is demonstrated

This step therefore sets the foundation for **governance and oversight design**, not merely ethical compliance.

---

## Practical Guidance

When addressing ethics, equity, and risk:

- prioritise plausible, context-specific scenarios over abstract principles
- involve diverse perspectives where possible
- document trade-offs explicitly
- resist treating ‚Äúefficiency‚Äù as a neutral good

Ethical design is not about eliminating all risk, but about **deciding responsibly where risk is acceptable and where it is not**.

---

## Summary

This step ensures that Capability-Driven Development does not treat ethics and equity as external constraints, but as **core design considerations**.

By addressing ethics, equity, and risk early, CDD helps ensure that AI-enabled systems:

- do not quietly entrench inequality
- remain accountable to those affected by their use
- can be defended ethically as well as technically

---

## Next Step

Continue to:

üëâ `04-governance-oversight.md`

That step translates ethical and risk considerations into concrete mechanisms for accountability, review, and control.
